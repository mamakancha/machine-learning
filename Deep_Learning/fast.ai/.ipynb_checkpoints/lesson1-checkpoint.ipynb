{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Neural networks are universal approximating machines. To fit the parameters for the function, we use Gradient Descent. \n",
    "\n",
    "We are able to make this function fast with the help of GPU since GPUs are based on mostly based on matrix operations (on pixels) which is also what we want for Deep learning. We need NVIDIA GPU as they support CUDA. Amazon provides us GPU instances called P2 instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the instances we use AWS cli. For this use the alias.sh file in the fast-ai course in documents folder and go to setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run the aws-alias.sh\n",
    "# | source aws-alias.sh\n",
    "# | aws-get-p2\n",
    "# this line will get the instance id for the p2 instance and save it in variable \n",
    "#`instanceId`\n",
    "# to start the instance\n",
    "# | aws-start. This will start the instance and queries for the ip and prints it out\n",
    "# | aws-ssh. This will then ssh into that instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are inside our aws instance. AWS has something called AMI (Amazon Machine Images). It is basically a snapshot of the computer at a particular instance of time. We can start our instance using a copy of that snapshot. In the script given, there was an AMI which already had all the things installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we type `jupyter notebook` in the aws ssh, it returns us the port which we should append with our ip address obtained from 'aws-start`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 notebooks can be run in parallel compeletely separate from each other. \n",
    "\n",
    "Now to prevent typing **source aws-alias.sh** everytime, add it to **.bashrc** which contains a set of commands that bash runs before starting\n",
    "\n",
    "# Getting started with Dogs vs Cats\n",
    "\n",
    "To get started with the dogs vs cats, first we run the p2 instance of aws and then **wget** to get the .ipynb file and the data.\n",
    "\n",
    "**The structuring of the files in data is very important**\n",
    "* keras expects that each class of the image be in separate folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are about 12000 images each of dogs and cats in the train folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Always test the model on a small sample first**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tells the jupyter notebook to display all the matplotlib graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path = \"data/dogscats/\"\n",
    "path = \"data/dogscats/sample/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tells the path to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using anaconda to install stuff. To install via anaconda ** conda install thing**. Sometimes conda installer not available, we will use pip.\n",
    "\n",
    "Now we would be using a pretrained neural network\n",
    "* V5516 -2014 winner\n",
    "* Inception-2015 winner\n",
    "* Resnet - 2016 winner\n",
    "We use VGG as it is the last \"simple\" model. Our script for VGG16 has already been downloaded.\n",
    "\n",
    "\n",
    "Now Keras runs on top of Theano/Tensorflow which convert our python code to CUDNN based code. Theano is sitting on top of CUDNN (CUDA deep neural network library).\n",
    "\n",
    "Tensorflow-> works well for multi GPUs\n",
    "Keras can easily be configured to use Tensorflow as backend instead of theano.\n",
    "\n",
    "To do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd ~/.keras/\n",
    "#vim keras.json\n",
    "# here change the backend to tensorflow\n",
    "# also change the th to tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change theano to use cpu instead of gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vim ~/.theanorc\n",
    "#Here change the cpu to gpu or vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dnn, we train in batches. We can't do all at time as it may be too large for the GPU's memory. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
